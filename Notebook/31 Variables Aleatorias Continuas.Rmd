---
title: ''
output: html_document
---

# Variables aleatorias continuas

**Variable aleatoria continua**. Una v.a. $X: \Omega \rightarrow \mathbb{R}$ es continua cuando su función de distribución $F_x: \mathbb{R} \rightarrow [0,1]$ es continua.

En este caso, $F_x(x)=Fx(x^{-})$ y, por este motivo, 

$$p(X=x) = 0 \mbox{ } \forall x \in \mathbb{R}$$

pero esto no significa que sean sucesos imposibles.

## Función de densidad

**Función de densidad**. Función $f: \mathbb{R} \rightarrow \mathbb{R}$ que satisface

* $f(x) \geq 0 \mbox{ } \forall x \in \mathbb{R}$

* $\int_{- \infty}^{+ \infty}dt = 1$

Una función de densidad puede tener puntos de discontinuidad.

Toda variable aleatoria $X$ con función de distribución 

$$F(x) = \displaystyle \int _{- \infty}^{x}dt \mbox{ } \forall x \in \mathbb{R} $$
para cualquier densidad $f$ es una v.a. continua 

Diremos entoncesque $f$ es una función de densidad de $X$. 
A partir de ahora consideraremos solamente las v.a. $X$ continuas que tienen función de densidad.

**Esperanza de una v.a. continua**. Sea $X$ v.a. continua con densidad $fx$. La esperanza de $X$ es 

$$E(X) = \displaystyle \int _{- \infty}^{\infty} x \cdot f_x(x)dx$$

Si el dominio $D_x$ de $X$ es un intervalo de extremos $a<b$, entonces

$$E(X) = \displaystyle \int _{a}^{b} x \cdot f_x(x)dx$$

Sea $g: D_x \rightarrow \mathbb{R}$ una función continua. Entonces, 

$$E(g(X)) = \displaystyle \int _{- \infty}^{\infty} g(x) \cdot f_x(x)dx$$

Si el dominio $D_x$ de $X$ es un intervalo de extremos $a<b$, entonces

$$E(g(X)) = \displaystyle \int _{a}^{b} g(x) \cdot f_x(x)dx$$

**Varianza de una v.a. continua**. Como en el caso discreto,

$$Var(X)=E((X-E(X))^2)$$
y se puede demostrar que

$$Var(X) = E(X^2)-(E(X))^2$$

## Distribuciones continuas

* Uniforme
* Exponencial 
* Normal
* Khi cuadrado
* t de Student
* F de Fisher

# Distribución Uniforme

Una v.a. continua $X$ tiene distribución uniforme sobre el intervalo real $[a,b]$ con  $a<b, X \sim U(a,b)$ si su función de densidad es 
$$fx(x) = \left\{ \begin{array}{rcl} \frac{1}{b-a} & \mbox{si } a \leq x \leq b 
 \\ 0 & \mbox{en cualquier otro caso}  \end{array}\right.$$
 
Modela el elegir un elemento del intervalo [a,b] de manera equiprobable

* El **dominio** de $X$ será $D_x = [a,b]$

* La **función de distribución** vendrá dada por

$$Fx(x) = \left\{ \begin{array}{rcl} 0 & \mbox{si } x < a  
 \\ \frac{x-a}{b-a} & \mbox{si  } a \leq x < b \\ 1 & \mbox{si  } x \geq b \end{array}\right.$$

* **Esperanza** $E(X) = \frac{a+b}{2}$

* **Varianza** $Var(X)= \frac{(b-a)^2}{12}$

### Ejemplo

Supongamos que $X \sim U([0,1])$ entonces podemos estudiar sus parametros.

```{r}
a = 0
b = 1

x = seq(-0.1, 1.1, 0.1)
plot(x, dunif(x, min = a, max = b))
plot(x, punif(x,a,b), type = "l")
qunif(0.5, a,b)
runif(10000, a,b) -> data
hist(data)


```

## Distribución Exponencial

Una v.a. $X$ tiene una distribución exponencial de parámetro $\lambda$, $X \sim Exp(\lambda)$, si su función de densidad es

$$fx(x)= \left\{ \begin{array}{rcl} 
0 & \mbox{si } x \leq 0 \\ \lambda \cdot e^{-\lambda x} &\mbox{si } x>0
\end{array}\right.$$

**Teorema**. Si tenemos un proceso de Poisson de parámetro $\lambda$ por unidad de tiempo, el tiempo que pasa entre dos sucesos consecutivos es una v.a. $Exp(\lambda)$.

**Propiedade de pérdida de memoria.** Si $X$ es v.a. $Exp(\lambda)$, entonces
$$p(X>s+t: X>s) = p(X>t) \mbox{ } \forall s,t >0$$
 
* El **dominio** de $X$ será $D_x= (0, \infty)$
* La **función de distribución** vendrá dada por

$$Fx(x) = \left\{ \begin{array}{rcl} 
0 & \mbox{si } x \leq 0 \\ 1-e^{-\lambda x} &\mbox{si } x>0
\end{array}\right.$$
 
* **Esperanza**. $E(X) = \frac{1}{\lambda}$
* **Varianza**. $Var(X)= \frac{1}{\lambda^{2}}$

# Distribución Normal

Una v.a. $X$ tiene distribución normal o gaussiana de paramétros $\mu$ y $\sigma$,
$X \sim  \mathcal{N}(\mu, \sigma)$ si su función de densidad es.

$$fx(x)= \frac{1}{\sqrt{2\pi \sigma}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \mbox{ } \forall x \in \mathbb{R}$$

La gráfica de $fx$ es conocida como la **Campana de Gauss**

Cuando $\mu=0$ y $\sigma=1$, diremos que la v.a. $X$ es **estandar** y la indicaremos usualmente como $Z$, la cual tendrá función de densidad

$$fz(z)= \frac{1}{2 \pi}e^{\frac{z^2}{2}} \mbox{  }\forall z \in \mathbb{R}$$

* **Esperanza** $E(X)= \mu$

* **Varianza** $Var(X= \sigma^2$

En particular, si Z sigue una distribución estándar,

* **Esperanza** $E(X)=0$

* **Varianza** $Var(X)=1$


**Estandarización de una v.a. normal** Si $X$ es una v.a. $\mathcal{N}(\mu, \sigma)$, entonces.

$$Z=\frac{X-\mu}{\sigma} \sim \mathcal{N} (0,1)$$

Las probabilidades de una normal estándar $Z$ determinan las de cualquier $X$ de tipo $\mathcal{N}(\mu, \sigma)$

$$p(X \leq x) = p \left(\frac{X-\mu}{\sigma} \leq \frac{x-\mu}{\sigma} \right) = p \left(Z \geq \frac{x-\mu}{\sigma} \right)$$

$Fz$ no tienen expresión conocida.

Se puede calcular con cualquier programa, como por ejemplo R, o bien a mano utilizando tablas de la $\mathcal{N}(0,1)$

Con las tablas se puede calcular tanto probabilidades como cuantiles.


# Otras distribuciones conocidas

## Xi cuadrado

* La distribución $\mathcal{X}^2_k$, donde $k$ representa los grados de libertad de la misma y que procede le la suma de los cuadrados de k distribuciones normales estándar independientes:

$$X=Z_1^{2}+Z_2^{2}+...+Z_k^2 \sim \mathcal{X}_k^2$$

## t de Student

* La dsitribución $t_k$ surge del problema de estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeña procede del cociente. (Para muestras pequeñas)

$$T = \frac{Z}{\sqrt{\mathcal{X}^2k / k}} \sim T_k$$

## F de Fisher 

* La distribución $F_{n1}, F_{n2}$ aparece frecuentemente como la distribución nula de una prueba estadística, especialemente en el análisis de la varianza (ANOVA). Viene definida como el cociente

$$F = \frac{\mathcal{X}^2_{n1}/n_1}{\mathcal{X}^2_{n2}/n_2}\sim F_{n1,n2}$$
